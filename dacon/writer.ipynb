{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소설 작가 분류 AI 경진대회.\n",
    "> 월간 데이콘 9 | 소설 문체 | NLP |Logloss\n",
    "\n",
    "[참고]\n",
    "- https://dacon.io/competitions/official/235670/codeshare/1901?page=2&dtype=recent&ptype=pub\n",
    "- https://www.kaggle.com/marcospinaci/0-335-log-loss-in-a-dozen-lines\n",
    "- https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "# nltk?\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics, preprocessing, pipeline, model_selection, naive_bayes\n",
    "from sklearn.metrics import log_loss  #?\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.pipeline import Pipeline #?\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer #?\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB #?\n",
    "from sklearn.calibration import CalibratedClassifierCV #?\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import time\n",
    "\n",
    "# keras\n",
    "from keras import backend as K #?\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',200)\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test_x.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['text'].str.replace('[^a-zA-Z0-9]',' ')\n",
    "Y_train = LabelEncoder().fit_transform(train['author'])\n",
    "y_train = train['author']\n",
    "X_test = test['text'].str.replace('^[a-zA-Z0-9]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 비율(문장안에 각 부호가 얼마나 있는지 확인)\n",
    "punctuations=[{\"id\":1,\"p\":\"[;:]\"},\n",
    "              {\"id\":2,\"p\":\"[,.]\"},\n",
    "              {\"id\":3,\"p\":\"[?]\"},\n",
    "              {\"id\":4,\"p\":\"[!]\"},\n",
    "              {\"id\":5,\"p\":\"[''\\']\"},\n",
    "              {\"id\":6,\"p\":\"[\"\"\\\"]\"},\n",
    "              {\"id\":7,\"p\":\"[:;,.?! \\' \"\" '' \\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p['p']\n",
    "    _train = [sentence.split() for sentence in train['text']]\n",
    "    train['punc_' + str(p['id'])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _train]\n",
    "        \n",
    "    _test = [sentence.split() for sentence in test['text']]\n",
    "    test['punc_' + str(p['id'])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 비율(문장 안에 각 부호가 얼마나 있는지)\n",
    "punctuations = [{\"id\":1, \"p\" : \"[;:]\"},\n",
    "                {\"id\":2, \"p\" : \"[,.]\"},\n",
    "                {\"id\":3, \"p\" : \"[?]\"},\n",
    "                {\"id\":4, \"p\" : \"[!]\"},\n",
    "                {\"id\":5, \"p\" : \"[‘’\\']\"},\n",
    "                {\"id\":6, \"p\" : \"[“”\\\"]\"},\n",
    "                {\"id\":7, \"p\" : \"[;:,.?!\\'“”‘’\\\"]\"}]\n",
    "\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [sentence.split() for sentence in train['text']]\n",
    "    train['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _train]\n",
    "\n",
    "    _test =  [sentence.split() for sentence in test['text']]\n",
    "    test['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- TfidfVectorizer\n",
    "- CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer - word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021010719:20\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-85af7db06859>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mgs_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mbest_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print('%04d%02d%02d%02d:%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour,start.tm_min))\n",
    "\n",
    "# tfidf_MNB_\n",
    "cv_scores=[]\n",
    "pred_full_test=0\n",
    "pred_train = np.zeros([train.shape[0],5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "    \n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                           ('tfidf',TfidfTransformer()),\n",
    "                           ('clf',MultinomialNB()),\n",
    "                          ])\n",
    "    parameters = {'vect__ngram_range':[(1,2)],\n",
    "                  'vect__max_df':(0.25,0.3),\n",
    "#                   'vect__min_df':[1],\n",
    "                  'vect__analyzer':['word'],\n",
    "                  'clf__alpha':[0.024, 0.031],\n",
    "                 }\n",
    "    \n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs =-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r'%(param_name, best_parameters[param_name]))\n",
    "        \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)   \n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "        \n",
    "print('cv socres:',cv_scores)\n",
    "print('Mean cv score',np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test/5\n",
    "\n",
    "train['tfidf_MNB_0'] = pred_train[:,0]\n",
    "train['tfidf_MNB_1'] = pred_train[:,1]\n",
    "train['tfidf_MNB_2'] = pred_train[:,2]\n",
    "train['tfidf_MNB_3'] = pred_train[:,3]\n",
    "train['tfidf_MNB_4'] = pred_train[:,4]\n",
    "\n",
    "test['tfidf_MNB_0'] = pred_full_test[:,0]\n",
    "test['tfidf_MNB_1'] = pred_full_test[:,1]\n",
    "test['tfidf_MNB_2'] = pred_full_test[:,2]\n",
    "test['tfidf_MNB_3'] = pred_full_test[:,3]\n",
    "test['tfidf_MNB_4'] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021010719:25\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv socre: [0.5976665473926792, 0.5975338255728783, 0.6173897988183089, 0.5963371295574684, 0.6096579691870107]\n",
      "Mean cv socre: 0.6037170541056691\n",
      "2021/01/07 19:25\n",
      "2021/01/07 19:26\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print('%04d%02d%02d%02d:%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour,start.tm_min))\n",
    "\n",
    "# tfidf_MNB_\n",
    "cv_scores=[]\n",
    "pred_full_test=0\n",
    "pred_train = np.zeros([train.shape[0],5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "    \n",
    "    # 위와 clf 부분 다름\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf',TfidfTransformer()),\n",
    "                          ('clf',CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method = 'isotonic')),\n",
    "                          ])\n",
    "    parameters = {'vect__ngram_range':[(1,2)],\n",
    "                 'vect__max_df': (0.4, 0.5),\n",
    "                 #'vect__min_df':[1],\n",
    "                  'vect__analyzer':['word'],\n",
    "                 #'clf__alpha' :(0.016, 0.018),\n",
    "                 }\n",
    "\n",
    "    \n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs = -1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r' %(param_name, best_parameters[param_name]))\n",
    "            \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index,:] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "    \n",
    "print('cv socre:',cv_scores)\n",
    "print('Mean cv socre:', np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test/5\n",
    "    \n",
    "train['tfidf_MNB_0'] = pred_train[:,0]\n",
    "train['tfidf_MNB_1'] = pred_train[:,1]\n",
    "train['tfidf_MNB_2'] = pred_train[:,2]\n",
    "train['tfidf_MNB_3'] = pred_train[:,3]\n",
    "train['tfidf_MNB_4'] = pred_train[:,4]\n",
    "\n",
    "test['tfidf_MNB_0'] = pred_full_test[:,0]\n",
    "test['tfidf_MNB_1'] = pred_full_test[:,1]\n",
    "test['tfidf_MNB_2'] = pred_full_test[:,2]\n",
    "test['tfidf_MNB_3'] = pred_full_test[:,3]\n",
    "test['tfidf_MNB_4'] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
