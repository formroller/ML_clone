{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소설 작가 분류 AI 경진대회.\n",
    "> 월간 데이콘 9 | 소설 문체 | NLP |Logloss\n",
    "\n",
    "[참고]\n",
    "- https://dacon.io/competitions/official/235670/codeshare/1901?page=2&dtype=recent&ptype=pub\n",
    "- https://www.kaggle.com/marcospinaci/0-335-log-loss-in-a-dozen-lines\n",
    "- https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "# nltk?\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics, preprocessing, pipeline, model_selection, naive_bayes\n",
    "from sklearn.metrics import log_loss  #?\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.pipeline import Pipeline #?\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer #?\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB #?\n",
    "from sklearn.calibration import CalibratedClassifierCV #?\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import time\n",
    "\n",
    "# keras\n",
    "from keras import backend as K #?\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',200)\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test_x.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['text'].str.replace('[^a-zA-Z0-9]',' ')\n",
    "Y_train = LabelEncoder().fit_transform(train['author'])\n",
    "y_train = train['author']\n",
    "X_test = test['text'].str.replace('^[a-zA-Z0-9]',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 비율(문장안에 각 부호가 얼마나 있는지 확인)\n",
    "punctuations=[{\"id\":1,\"p\":\"[;:]\"},\n",
    "              {\"id\":2,\"p\":\"[,.]\"},\n",
    "              {\"id\":3,\"p\":\"[?]\"},\n",
    "              {\"id\":4,\"p\":\"[!]\"},\n",
    "              {\"id\":5,\"p\":\"[''\\']\"},\n",
    "              {\"id\":6,\"p\":\"[\"\"\\\"]\"},\n",
    "              {\"id\":7,\"p\":\"[:;,.?! \\' \"\" '' \\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p['p']\n",
    "    _train = [sentence.split() for sentence in train['text']]\n",
    "    train['punc_' + str(p['id'])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _train]\n",
    "        \n",
    "    _test = [sentence.split() for sentence in test['text']]\n",
    "    test['punc_' + str(p['id'])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 비율(문장 안에 각 부호가 얼마나 있는지)\n",
    "punctuations = [{\"id\":1, \"p\" : \"[;:]\"},\n",
    "                {\"id\":2, \"p\" : \"[,.]\"},\n",
    "                {\"id\":3, \"p\" : \"[?]\"},\n",
    "                {\"id\":4, \"p\" : \"[!]\"},\n",
    "                {\"id\":5, \"p\" : \"[‘’\\']\"},\n",
    "                {\"id\":6, \"p\" : \"[“”\\\"]\"},\n",
    "                {\"id\":7, \"p\" : \"[;:,.?!\\'“”‘’\\\"]\"}]\n",
    "\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [sentence.split() for sentence in train['text']]\n",
    "    train['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _train]\n",
    "\n",
    "    _test =  [sentence.split() for sentence in test['text']]\n",
    "    test['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- TfidfVectorizer\n",
    "- CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer - word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021011109:22\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:    5.8s remaining:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    6.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.3\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-35694462ffcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mpred_test_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mpred_test_y2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mpred_full_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_full_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpred_test_y2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mpred_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_test_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \"\"\"\n\u001b[0;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'predict_proba'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_estimator_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1878\u001b[0m                    \"be removed in 0.24.\")\n\u001b[0;32m   1879\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1881\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m_word_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;34m\"\"\"Turn tokens into a sequence of n-grams after stop words filtering\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# handle stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1\n",
    "start = time.localtime()\n",
    "print('%04d%02d%02d%02d:%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour,start.tm_min))\n",
    "\n",
    "# tfidf_MNB_\n",
    "cv_scores=[]\n",
    "pred_full_test=0\n",
    "pred_train = np.zeros([train.shape[0],5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "    \n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                           ('tfidf',TfidfTransformer()),\n",
    "                           ('clf',MultinomialNB()),\n",
    "                          ])\n",
    "    parameters = {'vect__ngram_range':[(1,2)],\n",
    "                  'vect__max_df':(0.25,0.3),\n",
    "#                   'vect__min_df':[1],\n",
    "                  'vect__analyzer':['word'],\n",
    "                  'clf__alpha':[0.024, 0.031],\n",
    "                 }\n",
    "    \n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs =-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r'%(param_name, best_parameters[param_name]))\n",
    "        \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)   \n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "        \n",
    "print('cv socres:',cv_scores)\n",
    "print('Mean cv score',np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test/5\n",
    "\n",
    "train['tfidf_MNB_0'] = pred_train[:,0]\n",
    "train['tfidf_MNB_1'] = pred_train[:,1]\n",
    "train['tfidf_MNB_2'] = pred_train[:,2]\n",
    "train['tfidf_MNB_3'] = pred_train[:,3]\n",
    "train['tfidf_MNB_4'] = pred_train[:,4]\n",
    "\n",
    "test['tfidf_MNB_0'] = pred_full_test[:,0]\n",
    "test['tfidf_MNB_1'] = pred_full_test[:,1]\n",
    "test['tfidf_MNB_2'] = pred_full_test[:,2]\n",
    "test['tfidf_MNB_3'] = pred_full_test[:,3]\n",
    "test['tfidf_MNB_4'] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021011109:22\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-21ef2dbf872e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mgs_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mbest_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1114\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m                         \u001b[0mfeature_counter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1117\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m                     \u001b[1;31m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2\n",
    "## MultinomiaaNB(alpha = 0.05 )\n",
    "\n",
    "\n",
    "start = time.localtime()\n",
    "print('%04d%02d%02d%02d:%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour,start.tm_min))\n",
    "\n",
    "# tfidf_MNB_\n",
    "cv_scores=[]\n",
    "pred_full_test=0\n",
    "pred_train = np.zeros([train.shape[0],5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "    \n",
    "    # 위와 clf 부분 다름\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf',TfidfTransformer()),\n",
    "                          ('clf',CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method = 'isotonic')),\n",
    "                          ])\n",
    "    # clf_apha 주석\n",
    "    parameters = {'vect__ngram_range':[(1,2)],\n",
    "                 'vect__max_df': (0.4, 0.5),\n",
    "                 #'vect__min_df':[1],\n",
    "                  'vect__analyzer':['word'],\n",
    "                 #'clf__alpha' :(0.016, 0.018),\n",
    "                 }\n",
    "\n",
    "    \n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs = -1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r' %(param_name, best_parameters[param_name]))\n",
    "            \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index,:] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "    \n",
    "print('cv socre:',cv_scores)\n",
    "print('Mean cv socre:', np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test/5\n",
    "    \n",
    "train['tfidf_MNB_0'] = pred_train[:,0]\n",
    "train['tfidf_MNB_1'] = pred_train[:,1]\n",
    "train['tfidf_MNB_2'] = pred_train[:,2]\n",
    "train['tfidf_MNB_3'] = pred_train[:,3]\n",
    "train['tfidf_MNB_4'] = pred_train[:,4]\n",
    "\n",
    "test['tfidf_MNB_0'] = pred_full_test[:,0]\n",
    "test['tfidf_MNB_1'] = pred_full_test[:,1]\n",
    "test['tfidf_MNB_2'] = pred_full_test[:,2]\n",
    "test['tfidf_MNB_3'] = pred_full_test[:,3]\n",
    "test['tfidf_MNB_4'] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021011109:21\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-453998044898>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mgs_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mbest_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    249\u001b[0m                            min(max_n + 1, n_original_tokens + 1)):\n\u001b[0;32m    250\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_original_tokens\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m                     \u001b[0mtokens_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspace_join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 3\n",
    "## MultinomiaaNB(alpha),BernoulliNB(alpha) 0.5 -> 0.02\n",
    "## 'vect__max_df'(0.4,0.5) -> (0.03, 0.4)\n",
    "\n",
    "start = time.localtime()\n",
    "print('%04d%02d%02d%02d:%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour,start.tm_min))\n",
    "\n",
    "# tfidf_MNB_\n",
    "cv_scores=[]\n",
    "pred_full_test=0\n",
    "pred_train = np.zeros([train.shape[0],5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "    \n",
    "    # 위와 clf 부분 다름\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf',TfidfTransformer()),\n",
    "                          ('clf',CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method = 'isotonic')),\n",
    "                          ])\n",
    "    # clf_apha 주석\n",
    "    parameters = {'vect__ngram_range':[(1,2)],\n",
    "                 'vect__max_df': (0.03, 0.4),\n",
    "                 #'vect__min_df':[1],\n",
    "                  'vect__analyzer':['word'],\n",
    "                 #'clf__alpha' :(0.016, 0.018),\n",
    "                 }\n",
    "\n",
    "    \n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs = -1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r' %(param_name, best_parameters[param_name]))\n",
    "            \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index,:] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "    \n",
    "print('cv socre:',cv_scores)\n",
    "print('Mean cv socre:', np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test/5\n",
    "    \n",
    "train['tfidf_MNB_0'] = pred_train[:,0]\n",
    "train['tfidf_MNB_1'] = pred_train[:,1]\n",
    "train['tfidf_MNB_2'] = pred_train[:,2]\n",
    "train['tfidf_MNB_3'] = pred_train[:,3]\n",
    "train['tfidf_MNB_4'] = pred_train[:,4]\n",
    "\n",
    "test['tfidf_MNB_0'] = pred_full_test[:,0]\n",
    "test['tfidf_MNB_1'] = pred_full_test[:,1]\n",
    "test['tfidf_MNB_2'] = pred_full_test[:,2]\n",
    "test['tfidf_MNB_3'] = pred_full_test[:,3]\n",
    "test['tfidf_MNB_4'] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/01/11/09/36\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.6028549775135393, 0.6172834527914917, 0.6170012178396332, 0.601880893347187, 0.619873449366342, 0.6028883684967539, 0.6184402906278125, 0.6182194795364018, 0.6022628815022257, 0.6202862741321026, 0.6024717063000936, 0.6188515838634626, 0.6168023618902848, 0.6011816322805669, 0.6210410841780997]\n",
      "Meanc cv score :  0.6120893102443998\n",
      "2021/01/11/09/36\n",
      "2021/01/11/09/37\n"
     ]
    }
   ],
   "source": [
    "# 4 \n",
    "## clf, SGDClassifier(loss = 'modified_huber', alpha = 0.00001, max_iter = 10000, tol=1e-4),method='sigmoid'\n",
    "\n",
    "start = time.localtime()\n",
    "print('%04d/%02d/%02d/%02d/%02d' % (start.tm_year, start.tm_mon,start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# stidf_CBNB_\n",
    "cv_csores =[]\n",
    "pred_ful_test = 0\n",
    "pred_train = np.zeros([train.shape[0],5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "    \n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase = False)),\n",
    "                         ('tfidf',TfidfTransformer()),\n",
    "                         ('clf',CalibratedClassifierCV(SGDClassifier(loss = 'modified_huber', alpha = 0.00001, max_iter = 10000, tol=1e-4),method='sigmoid')),\n",
    "#                            ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "                           ])\n",
    "    parameters = {'vect__ngram_range':[(1,2)],\n",
    "#                  'vect__max_df':(0/03, 0.4),\n",
    "#                   'vect__min_df':[1],\n",
    "                  'vect__analyzer':['word'],\n",
    "#                   'clf__alpha':(0.016,0.018)\n",
    "                 }\n",
    "    \n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs = -1, verbose = 1, cv =2)\n",
    "    gs_clf.fit(dev_X,dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r'%(param_name, best_parameters[param_name]))\n",
    "        \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, :] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "    \n",
    "print('cv score : ', cv_scores)\n",
    "print('Meanc cv score : ', np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test/5\n",
    "\n",
    "train[\"tfidf_CBNB_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CBNB_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CBNB_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CBNB_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CBNB_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CBNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CBNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CBNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CBNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CBNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print('%04d/%02d/%02d/%02d/%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print('%04d/%02d/%02d/%02d/%02d' % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_L_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_L_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_L_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_L_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_L_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_L_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_L_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_L_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_L_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_L_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorzier - word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "                  'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_MNB_0\"] = pred_train[ : , 0]\n",
    "train[\"count_MNB_1\"] = pred_train[ : , 1]\n",
    "train[\"count_MNB_2\"] = pred_train[ : , 2]\n",
    "train[\"count_MNB_3\"] = pred_train[ : , 3]\n",
    "train[\"count_MNB_4\"] = pred_train[ : , 4]\n",
    "test[\"count_MNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_MNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_MNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_MNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_MNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CMNB_0\"] = pred_train[ : , 0]\n",
    "train[\"count_CMNB_1\"] = pred_train[ : , 1]\n",
    "train[\"count_CMNB_2\"] = pred_train[ : , 2]\n",
    "train[\"count_CMNB_3\"] = pred_train[ : , 3]\n",
    "train[\"count_CMNB_4\"] = pred_train[ : , 4]\n",
    "test[\"count_CMNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CMNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CMNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CMNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CMNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 \n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CBNB_0\"] = pred_train[ : , 0]\n",
    "train[\"count_CBNB_1\"] = pred_train[ : , 1]\n",
    "train[\"count_CBNB_2\"] = pred_train[ : , 2]\n",
    "train[\"count_CBNB_3\"] = pred_train[ : , 3]\n",
    "train[\"count_CBNB_4\"] = pred_train[ : , 4]\n",
    "test[\"count_CBNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CBNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CBNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CBNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CBNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 \n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CH_0\"] = pred_train[ : , 0]\n",
    "train[\"count_CH_1\"] = pred_train[ : , 1]\n",
    "train[\"count_CH_2\"] = pred_train[ : , 2]\n",
    "train[\"count_CH_3\"] = pred_train[ : , 3]\n",
    "train[\"count_CH_4\"] = pred_train[ : , 4]\n",
    "test[\"count_CH_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CH_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CH_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CH_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CH_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_L_0\"] = pred_train[ : , 0]\n",
    "train[\"count_L_1\"] = pred_train[ : , 1]\n",
    "train[\"count_L_2\"] = pred_train[ : , 2]\n",
    "train[\"count_L_3\"] = pred_train[ : , 3]\n",
    "train[\"count_L_4\"] = pred_train[ : , 4]\n",
    "test[\"count_L_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_L_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_L_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_L_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_L_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer - char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 3)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_MNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_MNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_MNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_MNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_MNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_MNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_MNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_MNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_MNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_MNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021/01/11 10:18\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   59.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   59.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   59.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score :  [0.7757343584422367, 0.7817726107614854, 0.813803751371872, 0.7902246807326817, 0.8351873041845956]\n",
      "Mean cv score :  0.7993445410985742\n",
      "2021/01/11 10:18\n",
      "2021/01/11 10:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIOJEAN\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "# 12\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CMNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CMNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CMNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CMNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CMNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CMNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CMNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CMNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CMNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CMNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 \n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CBNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CBNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CBNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CBNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CBNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CBNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CBNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CBNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CBNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CBNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CH_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CH_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CH_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CH_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CH_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CH_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CH_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CH_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CH_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CH_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_L_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_L_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_L_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_L_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_L_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_L_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_L_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_L_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_L_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_L_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer - chaar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 3)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_MNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_MNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_MNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_MNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_MNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_MNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_MNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_MNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_MNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_MNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CMNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_CMNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_CMNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_CMNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_CMNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_CMNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CMNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CMNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CMNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CMNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CBNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_CBNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_CBNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_CBNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_CBNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_CBNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CBNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CBNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CBNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CBNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 \n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CH_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_CH_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_CH_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_CH_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_CH_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_CH_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CH_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CH_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CH_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CH_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_L_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_L_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_L_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_L_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_L_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_L_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_L_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_L_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_L_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_L_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer - char_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 4)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_MNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_MNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_MNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_MNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_MNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_MNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_MNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_MNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_MNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_MNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CMNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CMNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CMNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CMNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CMNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CMNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CMNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CMNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CMNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CMNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CBNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CBNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CBNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CBNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CBNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CBNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CBNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CBNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CBNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CBNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 5)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CH_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CH_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CH_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CH_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CH_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CH_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CH_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CH_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CH_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CH_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 5)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_L_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_L_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_L_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_L_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_L_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_L_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_L_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_L_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_L_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_L_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer - char_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 4)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_MNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_MNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_MNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_MNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_MNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_MNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_MNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_MNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_MNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_MNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CMNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_CMNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_CMNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_CMNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_CMNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_CMNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CMNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CMNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CMNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CMNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CBNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_CBNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_CBNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_CBNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_CBNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_CBNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CBNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CBNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CBNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CBNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CH_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_CH_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_CH_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_CH_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_CH_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_CH_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CH_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CH_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CH_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CH_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30\n",
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_L_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_L_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_L_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_L_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_L_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_L_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_L_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_L_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_L_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_L_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"'\",\"'\")\n",
    "    signs = set(';:,.?!\\' \"\" '' \\\"')\n",
    "    prods = set(text) % signs\n",
    "    if not prods:\n",
    "        return text\n",
    "    \n",
    "    for sign in prods:\n",
    "        text = text.replcae(sign,' {} '.format(sign))\n",
    "    return text\n",
    "\n",
    "def creat+_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "        ngrams = []\n",
    "        for n in range(2, n_gram_max+1):\n",
    "            for w_index in range(len(q)-n+1):\n",
    "                ngrams.append('--'.join(q[w_index:windex+n]))\n",
    "        return q + ngrams\n",
    "    \n",
    "    docs= []\n",
    "    for docs in df.text:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "        \n",
    "    return docs\n",
    "\n",
    "docs = create_docs(train)\n",
    "tokenizer = Tokenizer(lower = False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_coints.items() if v >=2])\n",
    "\n",
    "tokenizer = Tokenizer(num_words = num_words, lower=False, filters='')\n",
    "tokenzier.fit_no_texts(docs)\n",
    "docs = tokenizer.text_to_sequences(docs)\n",
    "\n",
    "maxlen = max([max(len(l) for l in docs)])\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "docs_test = create_docs(test)\n",
    "docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "docs_test = pad_sequences(sequences = docs_test, maxlen=maxlen)\n",
    "\n",
    "xtrain_pad = docs\n",
    "xtest_pad = docs_test\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20\n",
    "\n",
    "def initFastText(embedding_dims, input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = input_dim, ouput_dim = embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "\n",
    "for dev_index, val_index in kf.split(xtrain_pad):\n",
    "    dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "    dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "    \n",
    "    model = initFastText(embedding_dims,input_dim)\n",
    "    model.fit(dev_X, dev_y,\n",
    "              batch_size=32, \n",
    "              epochs=40, \n",
    "              verbose=1, \n",
    "              validation_data=(val_X, val_y),\n",
    "              callbacks=[earlyStopping])\n",
    "    \n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(xtest_pad)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print('')\n",
    "    print('')    \n",
    "    print('')    \n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5 \n",
    "\n",
    "train[\"ff_0\"] = pred_train[:,0]\n",
    "train[\"ff_1\"] = pred_train[:,1]\n",
    "train[\"ff_2\"] = pred_train[:,2]\n",
    "train[\"ff_3\"] = pred_train[:,3]\n",
    "train[\"ff_4\"] = pred_train[:,4]\n",
    "test[\"ff_0\"] = pred_full_test[:,0]\n",
    "test[\"ff_1\"] = pred_full_test[:,1]\n",
    "test[\"ff_2\"] = pred_full_test[:,2]\n",
    "test[\"ff_3\"] = pred_full_test[:,3]\n",
    "test[\"ff_4\"] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "nb_words = 10000\n",
    "\n",
    "texts_1 = []\n",
    "for text in train['text']:\n",
    "    texts_1.append(text)\n",
    "    \n",
    "test_texts_1 = []\n",
    "for text in test['text']:\n",
    "    test_texts_1.append(text)\n",
    "    \n",
    "    \n",
    "tokenizer = Tokenzier(num_words = nb_words)\n",
    "tokenizer.fit_on_texts(texts_1)\n",
    "sequences_1 = tokenzier.texts_to_sequences(texts_1)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "test_sequences_1 = tokenzier.texts_to_sequences(test_texts_1)\n",
    "\n",
    "x_train_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "xtest_pad = pad_sequences(test_sequences_1, maxlen = max_len)\n",
    "del test_sequences_1\n",
    "del sequences_1\n",
    "nb_words_cnt = min(nb_words, len(word_index)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initNN(nb_words_cnt, max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt, 32, input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,5,padding='valid', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPolling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "\n",
    "for dev_index, val_index in kf.split(xtrain_pad):\n",
    "    dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "    dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "    \n",
    "    model = initNN(nb_words_cnt, max_len)\n",
    "    model.fit(dev_X, dev_y,\n",
    "              batch_size=32,\n",
    "              epochs=3,\n",
    "              verbose=1,\n",
    "              validation_data=(val_X, val_y),\n",
    "              callbacks=[earlyStopping])\n",
    "    \n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(xtest_pad)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5 \n",
    "\n",
    "train[\"nn_0\"] = pred_train[:,0]\n",
    "train[\"nn_1\"] = pred_train[:,1]\n",
    "train[\"nn_2\"] = pred_train[:,2]\n",
    "train[\"nn_3\"] = pred_train[:,3]\n",
    "train[\"nn_4\"] = pred_train[:,4]\n",
    "\n",
    "test[\"nn_0\"] = pred_full_test[:,0]\n",
    "test[\"nn_1\"] = pred_full_test[:,1]\n",
    "test[\"nn_2\"] = pred_full_test[:,2]\n",
    "test[\"nn_3\"] = pred_full_test[:,3]\n",
    "test[\"nn_4\"] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 및 검증\n",
    "\n",
    "### Model Tunning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.localtime()\n",
    "print('%04d/%02d/%02d/ %02d:%02' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "\n",
    "# Final Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X3 = None, seed_val=0, child = 1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta']= 0.1\n",
    "    parma['max_depth'] = 5\n",
    "#     param['silent']=1\n",
    "    param['num_class'] = 5\n",
    "    param['eval_metric']= 'mlogloss'\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "    \n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label = train_y)\n",
    "    \n",
    "    if test_y in not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label = test_y)\n",
    "        watchlist = [(xgtrain,'train'), (xgtest,'test')]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "        \n",
    "    else: \n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "        \n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(train ,test, Y_train):\n",
    "    drop_columns = ['index', 'text']\n",
    "    x_train = train.drop(drop_columns+['author'], axis = 1)\n",
    "    x_test = test.drop(drop_columns, axis = 1)\n",
    "    y_train =Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits = 5, shuffle=True, random_state = 32143233)\n",
    "    cv_scores=[]\n",
    "    pred_full_test=0\n",
    "    pred_train=np.zeros([x_train.shape[0],5])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y ,x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index, : ] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "        \n",
    "    print('cv score : ', cv_scores)\n",
    "    print('Mean cv score : ', np.mean(cv_scores))\n",
    "    return pred_full_test/5\n",
    "result = do(train, test, Y_train)\n",
    "\n",
    "end = time.localtime()\n",
    "print('%04d/%02d/%02d %02d:%02d' % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print('%04d/%02d/%02d %02d:%02d' % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
