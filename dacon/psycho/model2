# =============================================================================
# # Model2 : Lgbm Ensemble with different features
# =============================================================================
# 1.libraries 
# pip install eli5  * permutation importance ()

import seaborn as sns
import matplotlib.pyplot as plt

from string import ascii_lowercase
from itertools import combinations

import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.ensemble import VotingClassifier #?
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score


import pandas as pd
import numpy as np

from string import ascii_lowercase
from itertools import combinations

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score

from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.ensemble import RandomForestRegressor
import eli5 
from eli5.sklearn import PermutationImportance 
# PermutationImportance
# 빠름/사용범위넓음/일관된 feature 중요도 측정 가능
# 변수가 굉장히 많아 feature selection을 통해 변수를 제거할 필요가 있는 경우 PermutationImportance 활용하면 도움
# https://hong-yp-ml-records.tistory.com/51
import matplotlib.pyplot as plt

import warnings
import gc #??
# gc (Garbage Collection)
# 생성된 객체들을 순회하며 해당 객체가 현재 쓰이는 곳이 없을 경우 자동으로 해제한다.
# 사용자는 gc가 쓸모없어진 객체들을 잘 해제할 수 있도록 레퍼런스 카운트에 신경써야 한다
# https://medium.com/dmsfordsm/garbage-collection-in-python-777916fd3189 
# https://wikidocs.net/13969
warnings.filterwarnings('ignore')

# 2.Loading Data
train = pd.read_csv('./train.csv')
test = pd.read_csv('./test_x.csv')
# 3. Feature Engineering
x_train = train.copy()
x_train.drop('voted', axis=1, inplace=True)
y_train = train['voted']

dataset = [x_train,test]

# 마키아밸리 테스트 FE
questions = [i for i in list(ascii_lowercase)[:20]]
answer = [('Q'+i+'A') for i in questions]

for data in dataset:
    data['T'] = data['QcA'] - data['QfA'] + data['QoA'] - data['QrA'] + data['QsA']
    data['V'] = data['QbA'] - data['QeA'] + data['QhA'] + data['QjA'] + data['QmA'] - data['QqA']
    data['M'] = -data['QkA']
    
flipping_columns = ['QeA','QfA','QkA','QqA','QrA']
for data in dataset:
    for flip in flipping_columns:
        data[flip] = 6-data[flip]
        
flipping_secret_columns = ['QaA','QdA','QgA','QiA','QnA']
for data in dataset:
    for flip in flipping_secret_columns:
        data[flip] = 6 - data[flip]
        
for data in dataset:
    data['delay'] = data[[('Q'+i+'E')for i in questions]].sum(axis=1)
    data['delay'] = data['delay']**(1/10)
    data['delay_vr'] = data['delay'].var()
    
Ancoms = list(combinations(answer, 2))
for data in dataset:
    for a,b, in Ancoms:
        data['mach_%s_dv_%s'%(a,b)] = data[a]/data[b]
        
for data in dataset:
    data['mach_var'] = data[answer].var(axis=1)
    
# 그 외 features
tps = ['tp01','tp02','tp03','tp04','tp05','tp06','tp07','tp08','tp09','tp10']
for data in dataset:
    for tp in tps:
        data[tp] = 7 - data[tp]
        
# tipi feature들을 일반적인 형태로 복구
for data in dataset:
    for tp in tps:
        data[tp] = data[tp].replace(0,np.nan)
        mean = data[tp].mean(axis=0)
        data[tp] = data[tp].replace(np.nan, mean)
        
# tp중 무응답 값들을 평균값으로 대체
for data in dataset:
    data['Ex'] = data['tp01'] - data['tp06']
    data['Ag'] = data['tp07'] - data['tp02']
    data['Con'] = data['tp03'] - data['tp08']
    data['Es'] = data['tp09'] - data['tp04']
    data['Op'] = data['tp05'] - data['tp10']
    
index = test['index']
for data in dataset:
    data.drop('index',axis=1,inplace=True)
    
for data in dataset:
    teenager_ox = 1*np.array(data['age_group']=='10s')
# 10대 여부가 투표에 영향을 미칠 수 있으므로 새로운 컬럼 생성

tpcoms = list(combinations(tps, 2))
for data in dataset:
    for a,b in tpcoms:
        data['tp_%s_dv_%s'%(a,b)]=data[a]/data[b]
#=> tp 값들끼리 나눈 feature들 생성

encoder=LabelEncoder()
needenco=['age_group','gender','race','religion']
for i in needenco:
    x_train[i] = encoder.fit_transform(x_train[i])
    test[i] = encoder.transform(test[i])
    
    
for data in dataset:
    data['Es_gender'] = data['Es']*data['gender']
    data['Con_gender'] = data['Con']*data['gender']
    data['Op_gender'] = data['Op'] * data['gender']
    
# EDA결과 성별에 따라 Emotinal Stability/Conscience/Open Minede가 투표 여부에 미치는 영향이 크다 판단되어 feature를 추가
# 정보 출처: https://www.sciencedirect.com/science/article/abs/pii/S0261379413001613

# 4. Feature Selection 1 & Model 2-1

def lgbm_rfe_4040(x_data, y_data, ratio=0.9, min_feats=40):
    feats = x_data.columns.tolist()
    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])
    while True:
        model = LGBMClassifier(objective = 'binary', num_iterations=10**4)
        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=4040)
        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)
        val_pred = model.predict_proba(x_val)
        val_pred = val_pred[:,1]
        score = roc_auc_score(y_val, val_pred)
        n_feats = len(feats)
        print(n_feats, score)
        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)
        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)        
        next_n_feats = int(n_feats * ratio)
        if next_n_feats < min_feats:
            break
        else:
            feats = feat_imp.iloc[:next_n_feats].index.tolist()
    return archive

    feats = x_data.columns.tolist()
    archive = pd.DataFrame(columns=['model','n_feats','feats','score'])
    while True:
        model = LGBMClassifier(objective='binary',num_iteration=10**4)
        x_train,x_val,y_train,y_val = train_test_split(x_data[feats],y_data, random_state=4040)
        model.fit(x_train, y_train, eval_set=[(x_val,y_val)], early_stopping_rounds=100,verbose=False)
        val_pred = model.predict_proba(x_val)
        val_pred = val_pred[:,1]
        score=roc_auc_score(y_val, val_pred)
        n_feats = len(feats)
        print(n_feats, score)
        archive=archive.append({'model':model,'n_feats':n_feats,'feats':feats,'score':score}, ignore_index = True)
        feat_imp=pd.Series(model.feature_importances_,index=False).sort_values(ascending=False)
        next_n_feats = int(n_feats * ratio)
        if next_n_feats < min_feats:
            break
        else:
            feats = feat_imp.iloc[:next_n_feats].index.tolist()
    return archive

lgbm_archive_4040=lgbm_rfe_4040(x_train,y_train)

model = LGBMClassifier(objective='binary',num_iteration=10**3)
x_train_1 = x_train[lgbm_archive_4040.iloc[7,2]]
model.fit(x_train_1, y_train)

pred_y1 = model.predict_proba(test[lgbm_archive_4040.iloc[7,2]])
pred_y1 = pred_y1[:,1]


# 5. Feature Selection 2 & Model 2-2
def lgbm_rfe_1234(x_data, y_data, ratio=0.9, min_feats=40):
    feats = x_data.columns.tolist()
    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])
    while True:
        model = LGBMClassifier(objective = 'binary', num_iterations=10**4)
        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=1234)
        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)
        val_pred = model.predict_proba(x_val)
        val_pred = val_pred[:,1]
        score = roc_auc_score(y_val, val_pred)
        n_feats = len(feats)
        print(n_feats, score)
        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)
        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)
        next_n_feats = int(n_feats * ratio)
        if next_n_feats < min_feats:
            break
        else:
            feats = feat_imp.iloc[:next_n_feats].index.tolist()
    return archive
lgbm_archive_1234 = lgbm_rfe_1234(x_train, y_train)


model2 = LGBMClassifier(objective='binary',num_iterations=10**3)
x_train_2 = x_train[lgbm_archive_1234.iloc[14,2]]
model2.fit(x_train_2, y_train)

pred_y2 = model2.predict_proba(test[lgbm_archive_1234.iloc[14,2]])
pred_y2 = pred_y2[:,1]

# 6. Feature Selection 3 & Model 2-3
def lgbm_rfe_99087(x_data, y_data,ratio=0.9,min_feats=40):
    feats = x_data.columns.tolist()
    archive = pd.DataFrame(columns=['model','n_feats','feats','score'])
    while True:
        model=LGBMClassifier(objective='binary', num_iterations=10**4)
        x_train,x_val,y_train,y_val = train_test_split(x_data[feats],y_data,random_state=99087)
        model.fit(x_train,y_train,eval_set=[(x_val,y_val)], early_stopping_rounds=100, verbose=False)
        
        val_pred = model.predict_proba(x_val)
        val_pred = val_pred[:,1]
        
        score = roc_auc_score(y_val, val_pred)
        n_feats = len(feats)
        print(n_feats, score)
        
        archive = archive.append({'model':model,'n_feats':n_feats, 'feats':feats, 'score':score}, ignore_index=True)
        feat_imp = pd.Series(model.feature_importances_, index = feats).sort_values(ascending=False)
        next_n_feats = int(n_feats * ratio)
        
        if next_n_feats < min_feats:
            break
        else:
            feats = feat_imp.iloc[:next_n_feats].index.tolist()
    return archive
        
lgbm_archive_99087 = lgbm_rfe_99087(x_train, y_train)

model3 = LGBMClassifier(objective='binary', num_iterations=10**3)
x_train_3 = x_train[lgbm_archive_99087.iloc[7,2]]
model3.fit(x_train_3, y_train)

pred_y3 = model3.predict_proba(test[lgbm_archive_99087.iloc[7,2]])
pred_y3 = pred_y3[:,1]


# 7. Feature Selection 4 & Model 2-4
def lgbm_rfe_42(x_data,y_data, ratio=0.9, min_feats=40):
    feats = x_data.columns.tolist()
    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])
    while True:
        model = LGBMClassifier(objective = 'binary', num_iterations=10**4)
        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=42)
        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=False)
        val_pred = model.predict_proba(x_val)
        val_pred = val_pred[:,1]
        score = roc_auc_score(y_val, val_pred)
        n_feats = len(feats)
        print(n_feats, score)
        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)
        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)
        next_n_feats = int(n_feats * ratio)
        if next_n_feats < min_feats:
            break
        else:
            feats = feat_imp.iloc[:next_n_feats].index.tolist()
    return archive

lgbm_archive_42 = lgbm_rfe_42(x_train, y_train)

model4 = LGBMClassifier(objective="binary", num_iterations= 10**3)
x_train_4 = x_train[lgbm_archive_42.iloc[8,2]]
model4.fit(x_train_4, y_train)

pred_y4 = model4.predict_proba(test[lgbm_archive_42.iloc[8,2]])
pred_y4 = pred_y4[:,1]

# 8. Ensemble
pred_all = (pred_y + pred_y2 + pred_y3 + pred_y4) * (1/4)
submission = pd.DataFrame({
    'index':index,
    'voted':pred_all})

# submission.to_csv('./model2.csv', index=False)
