import os
os.getcwd()
os.chdir('./porto')
# =============================================================================
# chapter4. 포르토 세구로 안전 운전자 예측 경진대회 
# =============================================================================
# 4.1 경진대회 소개

문제 유형 : Binary Classification(이중 클래스 분류)
평가 척도 : Normalized Gini Coefficient(정규화 지니 계수)

-> 이번 대회에서, 여러분은 운전자가 내년에 자동차 보험 청구를 진행할 확률을 예측하는 모델을 개발하게 된다.

# 4.2 경진대회 주최자의 동기
보다 정확한 안전 운전자 예측 모델을 통해 자사 고객에게 합리적인 보험금을 청구하고자 함.

# 4.3 정규화 지니 계수
지니 계수(Gini Coefficient)는 불균형의 정고를 나타내는 통계학적 지수.
해당 지표는 경제 분야에서 소득격차별 부의 불균형의 정도를 나타내는데 대표적으로 사용된다

모든 경제 인구를 소득순으로 정렬한 후에, 그들의 누적 소득의 합을 그린 그래프를 로렌츠 곡선이라 한다.
모든 경제 인구에게 소득이 균등하게 배분되었을 경우, 45도의 직선이 그려지지만,
현실에서는 부의 불균형으로 인해 로렌츠 곡선과 같은 아래로 볼록한 곡선을 따르게 된다.

# Prediction (예측)
predictions = [0.9,0.3,0.8,0.75,0.65,0.6,0.78,0.7,0.05,0.4,0.4,0.05,0.5,0.1,0.1]
# Actual (정답)
actual = [1,1,1,1,1,1,0,0,0,0,0,0,0,0,0]

import numpy as np

파이썬으로 구현된 지니 계수 함수를 사용한다.
# 4-1 지니 계수를 계산하는 파이썬 함수

def gini(actual, pred):
    assert (len(actual) == len(pred))
    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype = np.float)
    all = all[np.lexsort((all[:,2], -1 * all[:,1]))]
    totalLosses = all[:,0].sum()
    giniSum = all[:,0].cumsum().sum() / totalLosses
    
    giniSum -= (len(actual) + 1) / 2.
    return giniSum / len(actual)


def gini_normalized(actual, pred):
    return gini(actual, pred) / gini(actual, actual)

# 4-2 지니 계수 정답값
# 위 예측값에 대한 실제 지니 계수, 최대 지니 계수, 정규화 지니 계수 값을 구한다.
gini_predictions = gini(actual, predictions)
gini_max = gini(actual, actual)
ngini = gini_normalized(actual, predictions)

print('Gini:%.3f, Max. Gini:%.3f, Normalized Gini:%.3f' % (gini_predictions, gini_max, ngini))

어떻게 지니 계수 0.189와 정규화 지니 0.630을 얻었는지 보자.

앞서 정의한 정답값을 예측값의 오름차순으로 정렬한다. 예측이 완벽하지 않기 때문에 정답값 0과 1이 섞인걸 볼 수 있다.

# 4-3 정답값을 예측값의 오름차순으로 정렬하는 코드
data = zip(actual, predictions)
sorted_data = sorted(data, key = lambda d : d[1])
sorted_actual = [d[0] for d in sorted_data]
print('Sorted Actual Values', sorted_actual)

# 4.4 주요 접근
* 이번 대회에서는 LightGBM을 사용
* 리더보드 순위를 올리기 위해 케라스 기반의 인공 신경망 모델을 학습에 모델의 다양성을 보탠다.
* 피처 엔지니어링 과정에서 XGBoost 모델을 사용한다는 것이 이번 경진대회 승자의 코드 차별화.

모델 라이브러리 : Keras =-2.1.1, lightgbm == 2.0.10, xgboost == 0.6a2

# 데이터
- 데이터는 철처히 익명화되어 있다. 
 (변수의 높낮이 파악 불가, 단순히 숫자만 주어짐.)
 
- EDA과정을 통해 각 변수와 데이터의 분포 분석.
- 훈련 데이터와 테스트 데이터의 분포를 비교해 효과적인 내부 교차 검증 프로세스를 구축하는데 참조한다.

# 피처 엔지니어링
- 이번 대회는 피처 엔지니어링이 핵심!

- 익명화된 데이터인 만큼 파생 변수 생성과 선별 과정은 철처히 실험 기반으로 수행하는 것을 권장.
- 변인을 최대한 줄인 상태에서 피처 엔지니어링을 수행

(승자의 코드)
- 결측값의 개수를 기반으로 파생 변수를 생성하고, 범주형 변수를 OnehotEncode해 새로운 변수 생성.
- 특정 변수 그룹을 문자열로 통합해 변수 그룹 내 조합을 나타내는 새로운 변수 생성
- 전체 데이터에서 변수 고유값별 빈도 등의 기초 통계값을 파생 변수로 사용

# 모델
- LightGBM 사용한다.
-> 테이블형 데이터 학습에 최적화된 GBDT 라이브러리(XGBoost, LightGBM,CatBoost)중 대표격이며,
  무엇보다 학습 속도가 빠르다는 이점이 있다.

# 앙상블
- 승자의 코드에서는 케라스 기반의 인공 신경망 모델을 추가로 학습한다.
- 단일 모델로는 인공 신경망 모델 성능은 우수하지 않으나, LightGBM 모델과 앙상블을 수행할때에 큰 성능 개선을 보인다.
- 모델의 다양성을 통해 점수 개선을 이루는 앙상블에 좋은 예이다.

# 4.5 데이터 준비하기
# 4.6 탐색적 데이터 분석
# 데이터 구조
훈련 데이터 train.7z와 테스트 데이터 test.7z는 7zip형태로 압축되어 있으며, 
용량은 각 17MB, 25MB이다

# 기초 통계로 살펴보기
import pandas as pd
import numpy as np

trn = pd.read_csv('train.csv', na_values=['-1','-1.0'])
tst = pd.read_csv('test.csv', na_values=['-1','-1.0'])

print(trn.shape, tst.shape)

->테스트 데이터에는 운전자의 보험 청구 여부를 나타내는 'target' 변수가 없기에 58개의 변수만 존재

trn.head()
-> 대부분의 변수가 수치형
-> 변수명이 'pd_ind_..'형태로 익명화

trn.info()
-> 모든 변수가 익명화
-> 데이터 타입은 int64 or float64로 통일
.. 고객 정보를 위해 익명화 실시한 것으로 예상
-> '_bin'로 끝나는 변수는 이진 변수
-> '_cat'로 끝나는 변수는 범주형 변수
-> '-1'은 결측값을 의미하며, 데이터를 불러오는 과정에서 NaN으로 지정한 결과, 몇몇 변수에서 결측값을 발견할 수 있다.

이번 대회에서 예측해야 할 타겟 변수('target')의 분포 확인

np.unique(trn['target'])
1.0 * sum(trn['target']) / trn.shape[0]

-> 타겟 변수의 고유값은 보험 청구 여부를 나타내는 [0,1]중 하나의 값을 가지는 이진변수
-> 전체 데이터중 3.6%의 운전자가 보험 청구를 진행했다.
-> 문제 특성상, 타겟 변수가 1일 확률이 매우 낮은 불균형한 데이터 이다.

# 시각화로 데이터 살펴보기
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

(다음 코드를 사용해 각 변수에 대해 막대 그래프 작성)

익명화된 변수를 데이터 타입 기준으로 이진, 범주형, 정수형, 소수형 변수 이렇게 4개 그룹으로 나눌 수 있다.
trn.columns
[for i in trn.columns]
